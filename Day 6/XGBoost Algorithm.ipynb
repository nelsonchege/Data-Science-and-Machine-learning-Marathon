{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae84966",
   "metadata": {},
   "source": [
    "## Introduction to XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe4077",
   "metadata": {},
   "source": [
    "it is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6b4fa",
   "metadata": {},
   "source": [
    "### XGBoost Features\n",
    "\n",
    "The library is laser-focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab27a4",
   "metadata": {},
   "source": [
    "### Model Features\n",
    "\n",
    "- Gradient Boosting algorithm also called gradient boosting machine including the learning rate.\n",
    "- Stochastic Gradient Boosting with sub-sampling at the row, - column, and column per split levels.\n",
    "- Regularized Gradient Boosting with both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb365531",
   "metadata": {},
   "source": [
    "### System Features\n",
    "\n",
    "- Parallelization of tree construction using all of your CPU cores during training.\n",
    "- Distributed Computing for training very large models using a cluster of machines.\n",
    "- Out-of-Core Computing for very large datasets that don’t fit into memory.\n",
    "- Cache Optimization of data structures and algorithm to make the best use of hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1755ab3",
   "metadata": {},
   "source": [
    "### Algorithm Features\n",
    "\n",
    "- Sparse Aware implementation with automatic handling of missing data values.\n",
    "- Block Structure to support the parallelization of tree construction.\n",
    "- Continued Training so that you can further boost an already fitted model on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d2133",
   "metadata": {},
   "source": [
    "### What is Boosting…?\n",
    "\n",
    "Boosting is an ensemble learning technique to build a strong classifier from several weak classifiers in series. Boosting algorithms play a crucial role in dealing with bias-variance trade-offs.\n",
    "\n",
    "##### types of boosting algorithms:\n",
    "\n",
    "1. AdaBoost (Adaptive Boosting)\n",
    "2. Gradient Boosting\n",
    "3. XGBoost\n",
    "4. CATBoost\n",
    "5. Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619554d",
   "metadata": {},
   "source": [
    "#### AdaBoost \n",
    "\n",
    "uses multiple iterations to generate a single composite strong learner. It creates a strong learner by iteratively adding weak learners. During each phase of training, a new weak learner is added to the ensemble, and a weighting vector is adjusted to focus on examples that were misclassified in previous rounds. The result is a classifier that has higher accuracy than the weak learner classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18330acd",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "The main objective of Gradient Boost is to minimize the loss function by adding weak learners using a gradient descent optimization algorithm. The generalization allowed arbitrary differentiable loss functions to be used, expanding the technique beyond binary classification problems to support regression, multi-class classification, and more.\n",
    "\n",
    "##### Gradient Boost has three main components.\n",
    "- Loss Function: The role of the loss function is to estimate how best is the model in making predictions with the given data. This could vary depending on the type of problem.\n",
    "- Weak Learner: Weak learner is one that classifies the data so poorly when compared to random guessing. The weak learners are mostly decision trees, but other models can be used in GBM.\n",
    "- Additive Model: It is an iterative and sequential process in adding the decision trees one step at a time. Each iteration should reduce the value of the loss function. A fixed number of trees are added, or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e777b7",
   "metadata": {},
   "source": [
    "##### XGBoost\n",
    "it is an extension to gradient boosted decision trees (GBM) and specially designed to improve speed and performance.\n",
    "###### XGBoost Features\n",
    "- Regularized Learning: The regularization term helps to smooth the final learned weights to avoid over-fitting. The regularized objective will tend to select a model employing simple and predictive functions.\n",
    "- Gradient Tree Boosting: The tree ensemble model cannot be optimized using traditional optimization methods in Euclidean space. Instead, the model is trained in an additive manner.\n",
    "- Shrinkage and Column Subsampling: Besides the regularized objective, two additional techniques are used to further prevent overfitting. The first technique is shrinkage introduced by Friedman. Shrinkage scales newly added weights by a factor η after each step of tree boosting. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each tree and leaves space for future trees to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d191b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
